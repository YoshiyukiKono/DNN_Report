# 第３章 情報理論

## 3-1
### 3-1-1 自己情報量
- 対数の底が２のとき、単位はビット(bit)
- 対数の底がネイピアのeのとき、単位は(nat)
```
I(x) = -log(P(x)) = log(W(x)) 
```

ｘ＝１の時、y = ０となる関数（ｘ＝０の時、y = ∞）

確率が小さいことが発生＝大きい情報量

### 3-1-2 ショノンエントロピー
- 自己情報量の期待値

```
H(x) = E(I(x))
= -E(log(P(x)))
= -Σ(P(x)log(P(x)))
```
## 3-2
### 3-2-1 カルバック・ライブラー ダイバージェンス
- 同じ事象・確率変数における異なる確率分布P,Qの違いを表す

```
D_KL(P||Q)
= E_x~P [log (P(x)/Q(x)) ]
= E_x~P [log P(x) - log Q(x) ]
```
### 3-2-2 交差エントロピー
- KLダイバージェンスの一部分を取り出したもの
- Qについての事故情報量をＰの分布で平均
```
H(P,Q) = H(P) + D_KL(P||Q)

H(P,Q) = -E_x~P logQ(x)
```
