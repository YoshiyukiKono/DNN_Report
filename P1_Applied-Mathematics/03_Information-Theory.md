# 第３章 情報理論

## 3-1
### 3-1-1 自己情報量
- 対数の底が２のとき、単位はビット(bit)
- 対数の底がネイピアのeのとき、単位は(nat)
```
I(x) = -log(P(x)) = log(W(x)) 
```

ｘ＝１の時、y = ０となる関数（ｘ＝０の時、y = ∞）

確率が小さいことが発生＝大きい情報量

#### エントロピー（平均情報量）
定義
```
H(X) := Σ_x {P(X) ・ I(X)}
= Σ_x {P(X) ・ -log P(X) }
```
##### 計算Tips
- bitでの回答の場合、logの底は2
- 対数で扱うため、分数表現を乗数表現に変化
```
1/n = n^(-1)
```
- 対数の定義から
```
logN^-1 = -1 logN
```
- Nとlogの底(2)が同じなら(Nが2なら)、log部分は「1」に置き換えられる

### 3-1-2 ショノンエントロピー
- 自己情報量の期待値

```
H(x) = E(I(x))
= -E(log(P(x)))
= -Σ(P(x)log(P(x)))
```
## 3-2
### 3-2-1 カルバック・ライブラー ダイバージェンス
- 同じ事象・確率変数における異なる確率分布P,Qの違いを表す

```
D_KL(P||Q)
= E_x~P [log (P(x)/Q(x)) ]
= E_x~P [log P(x) - log Q(x) ]
```
### 3-2-2 交差エントロピー
- KLダイバージェンスの一部分を取り出したもの
- Qについての事故情報量をＰの分布で平均
```
H(P,Q) = H(P) + D_KL(P||Q)

H(P,Q) = -E_x~P logQ(x)
```
