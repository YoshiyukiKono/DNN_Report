# ロジスティック回帰


## 分類問題

## シグモイド関数

## 最尤推定

尤度関数を最大化するようなパラメータを選ぶ推定方法

### ベルヌーイ分布

### 同時確率
- ロジスティック回帰では、*同時確率を利用*
- あるデータが得られた時、それが同時に得られる確率
- 確率変数は独立であることを仮定すると、*それぞれの確率の掛け算*となる

### 尤度関数 likelihood function

ある前提条件に従って結果が出現する場合に、逆に観察結果からみて前提条件が「何々であった」と推測する尤もらしさ（もっともらしさ）を表す数値を、「何々」を変数とする関数として捉えたもの。単に「尤度」という場合と基本、同意。

あるモデルのもとでそのデータが得られる確率

n回の試行で、ベルヌーイ分布の事象y（コインの表）がｘ回発生（確率θと置く）した場合の尤度（関数）
```
θ^x * (1-θ)^(n-x)
```
別式との比較
```
P(y) = p^y * (1-p)^(1-y)
```

### ロジスティック回帰モデルの最尤推定

```
P(y|w) = Π { p_i^y_i( 1 - p_i)^(1-y_i) }
       = Π σ(w^T x_i)^y_i(1 - σ(W^t * x_i))^`-y_i
```

#### 対数尤度関数
- ロジスティックでは、回帰対数尤度関数を用いる（微分計算の簡便化）
- 確率の値は`[0,1]`であるため、データ数が多いとき、尤度は桁が小さくなりアンダーフローを起こす恐れがある（積を和に変換しアンダーフローを回避）
- 対数尤度関数が最大になる点と尤度関数が最大になる点は同じ（対数関数は単調増加）
- 尤度関数にマイナスをかけたものを最小化（「最小2乗法の最小化」と合わせる＝最小化問題に変換することで既存の最適化手法を使用しやすくする）

### 勾配降下法 Gradient descent

対数尤度関数を係数とバイアスに関して微分

### 確率的勾配降下法 SGD Stocastic Gradient descent
- データを一つずつランダムに(「確率的」に)選んでパラメータを更新
- 勾配降下法でパラメータを1回更新するのと同じ計算量でパラメータをn回更新でき

### 性能評価指標

##### 正解率

(TP + TN) / (TP + FP + TN + FN)

##### 再現率 Recall

TP / (TP + FN)

- 使用例：病気の診断（除外診断）

- 感度：Sinsitibity

##### 適合率 Precision

TP / (TP + FP)

- 使用例：スパムメール

##### F値

2 x {(適合率 X 再現率)/(適合率 + 再現率)}

- PrecisionとRecallの調和平均
- F値が高ければ、適合率、再現率ともに高い

##### 特異度 Specificity

TN / (FP + TN)

- 感度/再現率/Recallとの兼ね合いで決まる
- 使用例：確定診断

## 決定境界
- 分類結果が切り替わる境目
- ロジスティック回帰の場合、決定境界は確率を計算した結果がちょうど50%になる箇所を指す。
