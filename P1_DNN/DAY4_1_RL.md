# DAY4 
## Section 1) 強化学習

### 1-1 強化学習とは

- 長期的に報酬を最大化できるように、環境の中で、行動を選択できるエージェントを作ることを目標とする。

- 行動の結果として、与えられる利益（報酬）をもとに、行動を決定する原理を改善していく仕組み

### 1-2 強化学習の応用例

- 環境：販売促進部門
- エージェント：プロフィールと購入履歴に基づいて、キャンペーンメールを送る顧客を決定するソフトウェア
- 行動：顧客ごとに送信、非送信の行動を選択する
- 報酬：
   - キャンペーンのコスト（負の報酬）
   - キャンペーンで生み出されると推測される売り上げ（正の報酬）

### 1-3 探索と利用のトレードオフ

- 不完全な知識を基に行動しながら、データを収集し最適な行動を見つけていく。

- 過去のデータを基に行動するだけでは、よりよい行動を見つけることができない（探索が足りていない）
- より良い行動を見つけるために、ランダムな探索的行動をしているだけでは、過去の経験を利用した成果を上げることができない


### 1-4 強化学習のイメージ

- エージェント
   - 方策関数： TT(s, a)
   - 行動価値関数：Q(s, a)

これらの関数を学習させる

### 1-5 強化学習の差分

- 教師あり学習や教師なし学習では、データに含まれるパターンを見つけ出し、データから予測することが目標

- 強化学習では、優れた方策を見つけることが目標

#### 強化学習の歴史

関数近似法とＱ学習を組み合わせる手法の登場により進歩

##### Ｑ学習
行動価値関数を、行動するごとに更新することにより学習を進める方法

##### 関数近似法
価値関数や方策関数を関数近似する手法

### 価値関数

- 状態価値関数
   - 環境の状態だけが価値を決める要因。
- 行動価値関数
   - 行動が価値を決める要因となる。状態と価値を組み合わせる。

ゲームに勝てそうかどうかを決める関数

### 方策関数
ある状態でどのような行動をとるかの確率を与える関数

π(s) = a

ゲームの一手を打つための関数

### 1-8 方策勾配法

π(s, a | Θ)

#### 方策反復法
方策をモデル化して最適化する手法


```
Θ^(t+1) = Θ^t + ε∇J(Θ)
```
- Θ：NNでいうところの重み

方策の良さ（J）は定義しなければならない

NNでは、誤差を小さくなるように学習をする
強化学習では、期待収益（報酬）が大きくなるように学習をする

定義方法
- 平均報酬
- 割引報酬和

- 上記の定義に対応して、行動価値関数:Q(s,a)の定義を行う。
- 方策勾配定理が成り立つ。


## Section 2) Alpha Go

## Section 3) 軽量化・高速化技術
### 3-1 モデル並列
### 3-2 データ並列
### 3-3 GPU
### 3-4 量子化
### 3-5 蒸留
### 3-6 プルーニング

## Section 4) 応用技術
### 4-1 
### 4-2 DenseNet
### 4-3 Layer正規化/Instance正規化
### 4-4 Wavenet
