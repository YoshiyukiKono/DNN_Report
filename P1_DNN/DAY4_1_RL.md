# DAY4 
## Section 1) 強化学習

### 1-1 強化学習とは

- 長期的に報酬を最大化できるように、環境の中で、行動を選択できるエージェントを作ることを目標とする。

- 行動の結果として、与えられる利益（報酬）をもとに、行動を決定する原理を改善していく仕組み

### 1-2 強化学習の応用例

- 環境：販売促進部門
- エージェント：プロフィールと購入履歴に基づいて、キャンペーンメールを送る顧客を決定するソフトウェア
- 行動：顧客ごとに送信、非送信の行動を選択する
- 報酬：
   - キャンペーンのコスト（負の報酬）
   - キャンペーンで生み出されると推測される売り上げ（正の報酬）

### 1-3 探索と利用のトレードオフ

- 不完全な知識を基に行動しながら、データを収集し最適な行動を見つけていく。

- 過去のデータを基に行動するだけでは、よりよい行動を見つけることができない（探索が足りていない）
- より良い行動を見つけるために、ランダムな探索的行動をしているだけでは、過去の経験を利用した成果を上げることができない


### 1-4 強化学習のイメージ

- エージェント
   - 方策関数： TT(s, a)
   - 行動価値関数：Q(s, a)

これらの関数を学習させる

### 1-5 強化学習の差分

- 教師あり学習や教師なし学習では、データに含まれるパターンを見つけ出し、データから予測することが目標

- 強化学習では、優れた方策を見つけることが目標

#### 強化学習の歴史

関数近似法とＱ学習を組み合わせる手法の登場により進歩

##### Ｑ学習
行動価値関数を、行動するごとに更新することにより学習を進める方法

##### 関数近似法
価値関数や方策関数を関数近似する手法

### 価値関数

- 状態価値関数
   - 環境の状態だけが価値を決める要因。
- 行動価値関数
   - 行動が価値を決める要因となる。状態と価値を組み合わせる。

ゲームに勝てそうかどうかを決める関数

### 方策関数
ある状態でどのような行動をとるかの確率を与える関数

π(s) = a

ゲームの一手を打つための関数

### 1-8 方策勾配法

π(s, a | Θ)

#### 方策反復法
方策をモデル化して最適化する手法


```
Θ^(t+1) = Θ^t + ε∇J(Θ)
```
- Θ：NNでいうところの重み

方策の良さ（J）は定義しなければならない

NNでは、誤差を小さくなるように学習をする
強化学習では、期待収益（報酬）が大きくなるように学習をする

定義方法
- 平均報酬
- 割引報酬和

- 上記の定義に対応して、行動価値関数:Q(s,a)の定義を行う。
- 方策勾配定理が成り立つ。


## Section 2) Alpha Go

### AlphaGo Lee


#### Policy Net

19 X 19マスの着手予想確率を出力する(２次元データ：SoftMax Layer)

#### Value Net

出力は、現局面の勝率を-1 ~ 1であらわしたもの（バイナリデータ：TanH Layer)

### 強化学習ステップ

- 教師あり学習によるRollOutPolicyとPolicyNetの学習
- 強化学習によるPolicyNetの学習
  - PolicyNetとPolicyPoolからランダムに選択されたPolicyNetと対局シミュレーションを行う。
- 強化学習によるValueNetの学習

#### RollOut Policy

PollicyNetよりも100倍速い

#### モンテカルロ木探索
強化学習手法。

### AlphaGo Zero
- 教師あり学習を行わず、強化学習のみで作成
- 特徴入力からヒューリスティックな要素（人間が役に立つと判断し付加した情報）を排除、石の配置のみとした。
- PolicyNetとValueNetをひとつのネットワークに統合
- Residual Netを導入
- モンテカルロ木探索からRollOutシミュレーションをなくした



#### Residual Net/Block
- ネットワークにショートカットを作る(勾配消失・爆発が起きづらくなる)
- 100層を超えるネットワークで安定した学習が可能になった(AlphaGo Zeroでは39層)
- Convolution > BatchNorm > ReLU > Concolution > BatchNorm > Add(ショートカットの入力) > ReLUという構造
- ショートカットにより、層数の違うNetworkのアンサンブル効果が得られる

AlphaGo Zeroでは、この後に、二つの出力（PolicyとValue）に経路を分割する。



##### AlphaGo Zeroでの工夫
###### Residual Blockに対する工夫
- Bottleneck
  - １層目で次元削減を行い、３層目で次元復元
- PreActivation
  - BatchNorm > ReLU > Convolution > BatchNorm > ReLU > Convolution > Add

###### Network構造に対する工夫
- WideResNet
   - ConvolutionのFilter数をK倍にしたResNet
   - GPUを効率的に利用
- PyramidNet
   - WideResNetに対する改良
   - 各層でFilter数を増やしていくResNet

## Section 3) 軽量化・高速化技術
### 3-1 モデル並列
### 3-2 データ並列
### 3-3 GPU
### 3-4 量子化
### 3-5 蒸留
### 3-6 プルーニング

## Section 4) 応用技術
### 4-1 
### 4-2 DenseNet
### 4-3 Layer正規化/Instance正規化
### 4-4 Wavenet
