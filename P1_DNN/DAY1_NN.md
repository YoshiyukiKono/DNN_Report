# DAY1 ニューラルネットワーク

## Section 1) 入力層～中間層
## Section 2) 活性化関数

### 中間層用の活性化関数
#### ステップ関数
しきい値を超えると発火する。出力は１か０


#### シグモイド（ロジスティック）関数

０～１を緩やかに変化する関数。

課題：大きな値では出力の変化が微小なため、勾配消失問題を引き起こすことがあった。

#### ReLU関数
もっとも使われている活性化関数。
勾配消失問題の回避とスパース化に貢献することでよい成果をもたらすことができる
### 出力層用の活性化関数

- 恒等写像：回帰
- シグモイド（ロジスティック）関数：二値分類
- ソフトマックス関数：他クラス分類

## Section 3) 出力層
### 3-1 誤差関数
### 3-2 出力層の活性化関数
## Section 4) 勾配降下法

### 確率的勾配降下法（ＳＧＤ）
ランダムに抽出したサンプルの誤差

メリット
- データが冗長な場合の計算コストの軽減
- 望まない局所極小解に収束するリスクの軽減
- オンライン学習が可能

### オンライン学習とは

### ミニバッチ勾配降下法
ランダムに分割したデータの集合（ミニバッチ）D_tに属するサンプルの平均誤差

メリット
- 確率的勾配降下法のメリットを損なわず、計算機の計算資源を有効利用でき
- CPUを利用したスレッド並列化や、GPUを利用したSIMD並列化

## Section 5) 誤差逆伝播法

産出された誤差を、出力層側から順に微分し、前の層、前の層へと伝播。
最小限の計算で各パラメータでの微分値を解析的に計算する手法

計算結果（＝誤差）から微分を逆算することで、不要な再帰的計算を避けて微分を算出できる。

数値微分のデメリットを回避することができる
