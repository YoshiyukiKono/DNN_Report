# DAY1 ニューラルネットワーク

## Section 1) 入力層～中間層

### 確認テスト
ディープラーニングとは
- 多数の中間層を持つニューラルネットワークを用いて、入力値から目的とする出力値に変換する数学モデルを構築する
- 重み(w)とバイアス(b)の最適化を目的とする

### 確認テスト
ニューラルネットワークの構造
- 入力層２ノード：x1, x2
- 中間層２層各３ノード:
  - u1(1),u2(1),u3(1) 
  - u1(2),u2(2),u3(2) 
- 出力層１ノード：y1

隣り合う層のノードは網羅的に結合される。（層ＡとＢ間の経路の数は、層Ａのノード数 x 層Ｂのノード数）

### ＮＮの対象とする問題
- 回帰：連続値
- 分類：離散値

### 確認テスト
ニューラルネットワークと分類問題との対応
- 入力：観測値
- 出力：クラス

### 確認テスト
Pythonによる総入力から出力を求める式の表現

総入力
```
u = np.dot(x, W) + b
```
総出力
```
z = functions.relu(u)
```

## Section 2) 活性化関数

### 中間層用の活性化関数
#### ステップ関数
しきい値を超えると発火する。出力は１か０


#### シグモイド（ロジスティック）関数

０～１を緩やかに変化する関数。

課題：大きな値では出力の変化が微小なため、勾配消失問題を引き起こすことがあった。

#### ReLU関数
もっとも使われている活性化関数。
勾配消失問題の回避とスパース化に貢献することでよい成果をもたらすことができる
### 出力層用の活性化関数

- 恒等写像：回帰
- シグモイド（ロジスティック）関数：二値分類
- ソフトマックス関数：他クラス分類

## Section 3) 出力層
### 3-1 誤差関数
### 3-2 出力層の活性化関数
## Section 4) 勾配降下法

### 確率的勾配降下法（ＳＧＤ）
ランダムに抽出したサンプルの誤差

メリット
- データが冗長な場合の計算コストの軽減
- 望まない局所極小解に収束するリスクの軽減
- オンライン学習が可能

### オンライン学習とは

### ミニバッチ勾配降下法
ランダムに分割したデータの集合（ミニバッチ）D_tに属するサンプルの平均誤差

メリット
- 確率的勾配降下法のメリットを損なわず、計算機の計算資源を有効利用でき
- CPUを利用したスレッド並列化や、GPUを利用したSIMD並列化

## Section 5) 誤差逆伝播法

産出された誤差を、出力層側から順に微分し、前の層、前の層へと伝播。
最小限の計算で各パラメータでの微分値を解析的に計算する手法

計算結果（＝誤差）から微分を逆算することで、不要な再帰的計算を避けて微分を算出できる。

数値微分のデメリットを回避することができる
