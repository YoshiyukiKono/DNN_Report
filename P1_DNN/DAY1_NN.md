# DAY1 ニューラルネットワーク

## Section 1) 入力層～中間層

### 確認テスト
ディープラーニングとは
- 多数の中間層を持つニューラルネットワークを用いて、入力値から目的とする出力値に変換する数学モデルを構築する
- 重み(w)とバイアス(b)の最適化を目的とする

### 確認テスト
ニューラルネットワークの構造
- 入力層２ノード：x1, x2
- 中間層２層各３ノード:
  - u1(1),u2(1),u3(1) 
  - u1(2),u2(2),u3(2) 
- 出力層１ノード：y1

隣り合う層のノードは網羅的に結合される。（層ＡとＢ間の経路の数は、層Ａのノード数 x 層Ｂのノード数）

### ＮＮの対象とする問題
- 回帰：連続値
- 分類：離散値

### 確認テスト
ニューラルネットワークと分類問題との対応
- 入力：観測値
- 出力：クラス

### 確認テスト
Pythonによる総入力から出力を求める式の表現

総入力
```
u = np.dot(x, W) + b
```
総出力
```
z = functions.relu(u)
```

## Section 2) 活性化関数

### 確認テスト
- 線形な関数
  - 加法性: f(x +y) = f(x) + f(y)
  - 斉次性: f(kx) = kf(x)
- 非線形な関数
  - 加法性・斉次性を満たさない

### 中間層用の活性化関数
#### ステップ関数
しきい値を超えると発火する。出力は１か０


#### シグモイド（ロジスティック）関数

０～１を緩やかに変化する関数。

課題：大きな値では出力の変化が微小なため、勾配消失問題を引き起こすことがあった。

#### ReLU関数
- もっとも使われている活性化関数。
- 勾配消失問題の回避とスパース化に貢献することでよい成果をもたらすことができる
- 0以下の入力は、常に0

#### 確認テスト
活性化関数のコードブロック
```
z = functions.relu(u)
```
```
def relu(x):
    np.maximum(0, x)
```



## Section 3) 出力層

### 3-1 誤差関数

#### 確認テスト
二乗和誤差(残差平方和)の計算式の意味
- 差が消しあわないように、それぞれのラベルが正の値になるように、二乗する。
- 誤差逆伝搬の計算で、誤差関数の微分を用いるがその際の計算を簡単にするため、（二乗した値の和を）二分の一にする

- 分類問題 - クロスエントロピー誤差
- 回帰問題 - mean squared error

### 3-2 出力層の活性化関数

- 恒等写像：回帰
  - なにもしない
  - 誤差関数：（平均）二乗誤差
- シグモイド（ロジスティック）関数：二値分類
  - 誤差関数：交差エントロピー
  
- ソフトマックス関数：他クラス分類
  - 誤差関数：交差エントロピー
  - 全体を足すと１になる確率数を出力する
  
#### 確認テスト
ソフトマックス処理解説
```
def softmax(x):
  if x.ndim == 2: # ミニバッチとしての処理の分岐
    x = x.T
    x = x - np.max(x, axis=0)
    y = np.exp(x) / np.sum(np.exp(x), axis=0)
    return y.T
  x = x - np.max(x) # オーバーフロー対策
  return np.exp(x) / np.sum(np.exp(x))　#　エクスポネンシャルの足し合わせた結果で、エクスポネンシャルを割る
```

#### 確認テスト
交差エントロピー処理解説
```
def cross_entropy_error(y, d):
  if y.ndim == 1:
    d = d.reshape(1, d.size)
    y = y.reshape(1, y.size)
  # 教師データがOne-hot-vectorの場合、正解ラベルのインデックスに変換
  if d.size == y.size:
    d = d.argmax(axis=1)
  batch_size = y.shape(0)
  return -np.sum(np.log(y[np.arrange(batch_size), d] + le-7)] / batch_size
```

`np.log(y[np.arrange(batch_size), d] + le-7)`がΣ_i=1~I{d_i log(y_i)}の部分

対数関数ではXが０に近づくと、yがマイナス無限大に近づく。`le-7`を足すことで、それを回避している（コンピュータ計算上の工夫）。

`y`は、０と１がならんだもの（正解が１）。`d`がＮＮが出力した値（正解の場所）。`d`が高いか低いかを計算して出力している。

## Section 4) 勾配降下法

ＮＮを学習させる手法。勾配降下法を利用してパラメータを最適化する。

### 確認テスト
勾配降下法コードブロック
```
W^(t+1) = w^(t) - ε∇E_n
```
```
network[key] -= learning_rate * gread[key]
```
```
∇E = ∂E/∂W = [ (∂E/∂W_1) ... (∂E/∂W_M)]
```
```
grad = backward(x, d, z1, y)
```
学習率εを適切に設定する
- 学習率 大：発散
- 学習率 小：時間がかかる。局所解になりかねない。


### 確率的勾配降下法（ＳＧＤ）
確率的勾配降下法
```
W^(t+1) = w^(t) - ε∇E_n
```
勾配降下法
- バッチ学習に対応
```
W^(t+1) = w^(t) - ε∇E
```
ランダムに抽出したサンプルの誤差

メリット
- データが冗長な場合の計算コストの軽減
- 望まない局所極小解に収束するリスクの軽減
- オンライン学習（バッチ学習とは異なり最初にデータをすべて準備する必要がない）が可能

#### 確認テスト
オンライン学習
- 学習データが入ってくる度に都度パラメータを更新し、学習を進めていく方法
  - バッチ学習では、一度にすべての学習データを使ってパラメータ更新を行う

### オンライン学習とは

### ミニバッチ勾配降下法
ランダムに分割したデータの集合（ミニバッチ）D_tに属するサンプルの平均誤差を用いる。

メリット
- 確率的勾配降下法のメリットを損なわず、計算機の計算資源を有効利用でき
- CPUを利用したスレッド並列化や、GPUを利用したSIMD(Single Instruction Multi Data)並列化

#### 確認テスト
```
W^(t+1) = w^(t) - ε∇E
```
- t:エポック
- w:重み
- １エポック進むごとに重みから`ε∇E`（誤差）を引く





## Section 5) 誤差逆伝播法

### 誤差勾配の計算
#### 数値微分
プログラムで微小な数値を生成し、疑似的に微分を計算する手法
- デメリット：計算量が大きい

#### 誤差逆伝播法
産出された誤差を、出力層側から順に微分し、前の層、前の層へと伝播。
最小限の計算で各パラメータでの微分値を**解析的**に計算する手法

計算結果（＝誤差）から微分を逆算することで、不要な再帰的計算を避けて微分を算出できる。

微分の連鎖率を利用

数値微分のデメリットを回避することができる

#### 確認テスト
誤差勾配計算コードブロック

すでに行っている計算を保持している部分
```
delta2 = functions.d_mean_squared_error(d, y)
...
delta1 = np.dot(delta2, W2, T) * functions.d_sigmoid(z1)
```

#### 確認テスト
```
(∂E/∂y)(∂y/∂u)
```
```
delta2 = functions.d_mean_squared_error(d, y)
```
```
(∂E/∂y)(∂y/∂u)(∂u/∂w_ji^(2))
```
```
grad['W2'] = np.dot(z1.T, delta2)
```

