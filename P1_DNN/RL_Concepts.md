# 強化学習コンセプト

### 代表的手法

#### 動的計画法
環境の完全なモデルを必要とする（マルコフ決定過程として）。

#### モンテカルロ法
遷移のサンプルを取得し、得られた収益を平均化することによって価値関数を推定する方法

#### TD学習(Temporal Difference Learning)
目標の価値と現在の価値のずれ（TD誤差）を修正していくことにより、価値関数を推定する方法

### TD学習手法
- Sarsaにおける行動価値関数の更新式：γ(S_(t+1), A_(t+1))を含む
- Q学習における行動価値関数の更新式：γmaxQ(S_(t+1), a')を含む

#### Sarsa
- 行動価値の小さい探索結果も反映されやすい
- 計算が不安定になりやすい
- 方策オン型の手法：行動決定方策と行動価値関数更新方策が同じ
- 方策勾配法（確率的方策πのパラメータθを勾配法で更新する方法）は用いられない
#### Q学習
- Ｑの更新が行動の決定方法に依存しない
- Sarsaよりも行動価値関数の収束が早くなることが多い（保証されてはいない）
- 行動決定方策と行動価値関数更新方策が異なる
- 行動価値関数を更新する際、行動価値の小さい探索結果は反映されにくい

## 方策勾配法
確率的方策πのパラメータθを勾配法で更新する方法

### 方策勾配定理式
TODO

- エピソード的タスク
- 連続タスク

#### REINFORCEアルゴリズム
