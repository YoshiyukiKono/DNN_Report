# DAY2 CNN
## Section 1) 勾配消失問題について

### 確認テスト
連鎖率の原理を使い、`dz/dx`を求めよ
```
1) z = t^2 
2) t = x + y
```
連鎖率により
```
dz/dx = dz/dt * dt/dx
```
式1)を微分
```
dz/dt = 2t
```
式2)を微分
```
dt/dx = 1
```
よって
```
dz/dx = 2t * 1 = 2t = 2(x +y)
```

#### 勾配消失問題の復習

縦方向：正解率
横方向：学習回数

誤差逆伝播法が下位層に進んでいくにつれて、勾配が緩やかになっていく。


ＮＮでは、入力層に近くなるほど、値が小さくなる。

シグモイド関数は、勾配消失問題を引き起こす代表的な関数。

シグモイド関数を微分すると、入力0の時に、出力が0.25になる


#### 確認テスト
シグモイド関数を微分したとき、入力値が０の時に最大値をとる。その値として正しいものを選べ
```
シグモイド関数を微分すると、入力0の時に、出力が0.25になる
```

#### 勾配消失の解決法
- 活性化関数の選択
- 重みの初期値設定
- バッチ正規化

##### 活性化関数の選択
###### ReLU関数

```
def relu(x):
  return np.maximum(0, x)
```
Relu関数の微分の出力は0か1になる。
- 1を掛けても値が小さくならない（勾配消失問題を解消）
- 重要でない重みは使われない（スパース化）

##### 重みの初期値設定
重みの初期値には、乱数を使う。

通常、標準正規分布（平均：０、分散：１）を基にした乱数を設定する。
###### Xavir
重みの要素を、前の層のノード数の平方根で除算した値。
```
network['W1'] = np.random.randn(input_layer_size, hidden_layer_size)/np.sqrt(input_layer_size)
```
S字カーブの関数に対して、上手く働く

###### He


重みの要素を、前の層のノード数の平方根で除算した値に対し√２を掛け合わせた値。
```
network['W1'] = np.random.randn(input_layer_size, hidden_layer_size)/np.sqrt(input_layer_size)*np.sqrt(2)
```

#### 確認テスト
重みの初期値に０を設定すると、どのような問題が発生するか？

全ての重みの値が均一に更新されるため、多数の重みを持つ意味がなくなる。

### バッチ正規化

ミニバッチ単位で、入力値のデータの偏りを抑制する手法

バッチ正規化の使いどころ：活性化関数に値を渡す前後に、バッチ正規化の処理をはらんだ層を加える

バッチ正規化の入力値：`u^(l) = w^(L)z(^l-1) + b^(l)` または`z`



#### ミニバッチのサイズ
- GPU:1～64枚の画像
- TPU:1～256枚の画像

#### 確認テスト
バッチ正規化の効果
- 中間層の重みの更新が安定し、学習スピードがアップする。
- 過学習を抑えることができる

#### ステップ
- 平均
- 分散
- 正規化
- 変倍・移動

#### 例題チャレンジ
ミニバッチプログラム
```
batch_x, batch_t = data_x[i:i_end], data_t[i:i_end]
```

## Section 2) 学習率最適化手法について

- 初期の学習率を大きく設定し、徐々に学習率を小さくしていく
- パラメータ毎に学習率を可変させる

### モメンタム
慣性:μを用いる

誤差をパラメータで微分したものと学習率の積を減産した後、現在の重みに前回の重みを減算した値と慣性の積を加算する。
（勾配降下法：誤差をパラメータで微分したものと学習率の積を減算する）

(移動平均に近い動きをする。勾配降下法のようにジグザグではなく、滑らかな軌跡を描く)

メリット
- 局所最適解にならず、大局的最適解を得やすい
- 最初は進みが遅いが、谷間についてから、最も低い位置（最適値）に行くまでの時間が早い。
### AdaGrad
誤差をパラメータで微分したものと再定義した学習率の積を減算する

メリット
- 勾配の緩やかな斜面に対して、最適値に近づける。

課題
- 学習率が徐々に小さくなるので、鞍点問題（大域的最適解に近づきにくい）を引き起こすことがあった。

### RMSProp
誤差をパラメータで微分したものと再定義した学習率の積を減算する

重みの更新式は、AdaGradと同じ

メリット
- 局所最適解にならず、大局的最適解を得やすい
- ハイパーパラメータの調整が必要な場合が少ない


### Adam
- モメンタムの、過去の勾配の指数関数的減衰平均
- RMSPropの、過去の勾配の２乗の指数関数的減衰平均
上記をそれぞれはらんだ最適化アルゴリズム

メリット
- モメンタムとRMSPropのメリットをもつ


## Section 3) 過学習について

### 正則化

ネットワークの自由度（層数、ノード数、パラメータ値など）を制約すること

正則化手法を利用して、過学習を抑制する。

### 正則化手法

###　正則化手法
- L1正則化、L2正則化
- ドロップアウト


### Weight Decay 荷重減衰
#### 過学習の原因
重みが大きい値をとることで、過学習が発生することがある。過大評価の発生
#### 過学習の解決策
誤差に対して、正則化項を加算することで、重みを抑制

#### L1正則化、L2正則化
- L1正則化（ラッソ回帰:p1ノルム）スパース推定
- L2正則化（リッジ回帰:p2ノルム）縮小推定

誤差関数にpノルムを加える

- p1ノルム：x + y (マンハッタン距離)
- p2ノルム：`√(x^2 + y^2)`(ユークリッド距離)

#### 確認テスト
機械学習で使われる線形モデル（線形回帰、主成分分析など）の正則化は、モデルの重みを制限することで可能になる。

リッジ回帰の特徴として正しいものを選択しなさい。
- ハイパーパラメータを大きな値にすると、すべての重みが限りなく０に近づく　（正解）
- ハイパーパラメータを０に設定すると、非線形回帰になる　誤：線形回帰になる
- バイアス項についても、正則化される。　誤：バイアス項は正則化されない
- リッジ回帰の場合、隠れ層に対して正則化項を加える　誤：隠れ層ではなく、誤差関数への正則化を加える（リッジ回帰に限らない正則化の特徴）

リッジ回帰：「正則化された線形回帰の一つで、線形回帰に「学習した重みの二乗の合計（L2正則化項）」を加えたもの」。L2正則化項による正則化では重みは完全に０にはならない性質があるため、説明変数が非常に多い時にはモデルの解釈が複雑になるという欠点がある。

#### 確認テスト

Ｌ１正則化のグラフのポイント

ラッソでは、一つの方向に対して０となる値を導き出すことができる

#### 例題チャレンジ
Ｌ２パラメータ正則化における最終的な勾配計算
```
grad += rate * param
```
- 微分形（選択肢のように、二乗がそのまま残るのはおかしい）
- 係数２は正則化の係数に吸収されても変わらないので用いない

#### 例題チャレンジ
Ｌ１パラメータ正則化における最終的な勾配計算
```
grad += rate * np.sign(param)
```
sign関数の出力
- -1: X < 0
-  0: x = 0
-  1: x > 0

#### 例題チャレンジ
データ拡張（画像のランダムな切り取り）
```
top = np.random.randint(0, h - crop_h)
left = np.random.randit(0, w - crop_w)
bottom = top + crop_h
right = left + crop_w

image = image[top:bottom, left:right, :]
```

### ドロップアウト
ランダムにノードを削除して学習させる

ドロップアウトは中間層の（より高等な判断を行う）後ろの層でを用いる

メリット
- データ量を変化させずに、異なるモデルを学習させていると解釈できる。


## Section 4) 畳み込みニュートラルネットワーク(CNN)の概念

CNNでは、次元間でつながりのあるデータを扱う

### 4-1 畳み込み層

#### 4-1-1 バイアス
#### 4-1-2 パディング
画像の周囲へデータを追加（0もしくは隣の値）

パディングを使うことで画像のサイズを変えずに畳み込み演算を行うことができる。

#### 4-1-3 ストライド
フィルターを移動する幅

#### 4-1-4 チャンネル



### 4-2 プーリング層

対象領域のMax値、または平均値を使って出力を得る。


#### 確認テスト
出力画像ファイル計算
```
O_h = (画像の高さ + 2 x パディング高さ - フィルタ高さ)/ストライド + 1

O_w = (画像の幅 + 2 x パディング幅 - フィルタ幅)/ストライド + 1
```

## Section 5) 最新のCNN

### AlexNet

### GoogLeNet
Inceptionモジュールを実装

#### Inceptionモジュール
複数の畳み込み層やPooling層から構成される小さなネットワーク
