# DAY2 CNN
## Section 1) 勾配消失問題について

### 確認テスト
連鎖率の原理を使い、`dz/dx`を求めよ
```
1) z = t^2 
2) t = x + y
```
連鎖率により
```
dz/dx = dz/dt * dt/dx
```
式1)を微分
```
dz/dt = 2t
```
式2)を微分
```
dt/dx = 1
```
よって
```
dz/dx = 2t * 1 = 2t = 2(x +y)
```

#### 勾配消失問題の復習

縦方向：正解率
横方向：学習回数

誤差逆伝播法が下位層に進んでいくにつれて、勾配が緩やかになっていく。


ＮＮでは、入力層に近くなるほど、値が小さくなる。

シグモイド関数は、勾配消失問題を引き起こす代表的な関数。

シグモイド関数を微分すると、入力0の時に、出力が0.25になる


#### 確認テスト
シグモイド関数を微分したとき、入力値が０の時に最大値をとる。その値として正しいものを選べ
```
シグモイド関数を微分すると、入力0の時に、出力が0.25になる
```

#### 勾配消失の解決法
- 活性化関数の選択
- 重みの初期値設定
- バッチ正規化

##### 活性化関数の選択
###### ReLU関数

```
def relu(x):
  return np.maximum(0, x)
```
Relu関数の微分の出力は0か1になる。
- 1を掛けても値が小さくならない（勾配消失問題を解消）
- 重要でない重みは使われない（スパース化）

##### 重みの初期値設定
重みの初期値には、乱数を使う。

通常、標準正規分布（平均：０、分散：１）を基にした乱数を設定する。
###### Xavir
重みの要素を、前の層のノード数の平方根で除算した値。
```
network['W1'] = np.random.randn(input_layer_size, hidden_layer_size)/np.sqrt(input_layer_size)
```
S字カーブの関数に対して、上手く働く

###### He


重みの要素を、前の層のノード数の平方根で除算した値に対し√２を掛け合わせた値。
```
network['W1'] = np.random.randn(input_layer_size, hidden_layer_size)/np.sqrt(input_layer_size)*np.sqrt(2)
```

#### 確認テスト
重みの初期値に０を設定すると、どのような問題が発生するか？

全ての重みの値が均一に更新されるため、多数の重みを持つ意味がなくなる。

### バッチ正規化

ミニバッチ単位で、入力値のデータの偏りを抑制する手法

バッチ正規化の使いどころ：活性化関数に値を渡す前後に、バッチ正規化の処理をはらんだ層を加える

バッチ正規化の入力値：`u^(l) = w^(L)z(^l-1) + b^(l)` または`z`



#### ミニバッチのサイズ
GPU:1~64枚の画像
TPU:1~256枚の画像

#### 確認テスト
バッチ正規化の効果
- 中間層の重みの更新が安定し、学習スピードがアップする。
- 過学習を抑えることができる

#### ステップ
- 平均
- 分散
- 正規化
- 変倍・移動

#### 例題チャレンジ
ミニバッチプログラム

## Section 2) 学習率最適化手法について
### モメンタム
### AdaGrad
### RMSProp
### Adam

## Section 3) 過学習について

### 正則化

### 正則化手法

#### L1正則化、L2正則化

#### ドロップアウト

### Weight Decay 荷重減衰



## Section 4) 畳み込みニュートラルネットワークの概念
## Section 5) 最新のCNN
